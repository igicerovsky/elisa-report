{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Useful code snippets for debugging"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Mask multiindex table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.stats import variation\n",
                "from itertools import combinations\n",
                "\n",
                "\n",
                "def mask_sample_cv(df_in, valid_pts, cv_threshold):\n",
                "    df = df_in[df_in['mask'].isna()]\n",
                "    display(df)\n",
                "    cv_min = cv_threshold  # variation(df['concentration'], ddof=1)\n",
                "    non_mask_idx = []\n",
                "    indices = df.index\n",
                "    # Reverse combinations order to break if `CV` < `cv_threshold`\n",
                "    for l in reversed(range(2, len(indices) + 1)):\n",
                "        for subset in combinations(indices, l):\n",
                "            comb = list(subset)\n",
                "            t = df.loc[comb]\n",
                "            display(t)\n",
                "            cv = variation(t['concentration'], ddof=1)\n",
                "            print(comb, cv)\n",
                "            if cv < cv_min:\n",
                "                non_mask_idx = comb\n",
                "                cv_min = cv\n",
                "                print(f'!!! min {cv}')\n",
                "        # break if CV drops below threshold\n",
                "        if cv_min < cv_threshold:\n",
                "            break\n",
                "\n",
                "    mask_idx = list(set(indices).symmetric_difference(non_mask_idx))\n",
                "    return mask_idx, non_mask_idx, cv_min"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "idx = pd.MultiIndex.from_product([['A'],\n",
                "                                  [1, 2, 3, 4]],\n",
                "                                 names=['col', 'row'])\n",
                "col = ['concentration', 'mask']\n",
                "\n",
                "dfm = pd.DataFrame([(10, np.nan), (11, np.nan),\n",
                "                   (6, '<8'), (16, np.nan)], idx, col)\n",
                "display(dfm)\n",
                "\n",
                "# display(dfm['mask'].isna())\n",
                "m_idx, _, _ = mask_sample_cv(dfm, 2, 0.2)\n",
                "display(m_idx)\n",
                "dfm.loc[m_idx, ['mask']] = \"cv-masked\"\n",
                "display(dfm)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Report dir handling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "\n",
                "def listdirs(rootdir):\n",
                "    dirs = []\n",
                "    for it in os.scandir(rootdir):\n",
                "        if it.is_dir():\n",
                "            dirs.append(it.path)\n",
                "            # print(it.path)\n",
                "    return dirs\n",
                "\n",
                "\n",
                "rootdir = './../reports/all/'\n",
                "dirs = listdirs(rootdir)\n",
                "dirs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def parse_dir_name(path_name):\n",
                "    if os.path.isdir(path_name):\n",
                "        path_name = os.path.basename(path_name)\n",
                "    else:\n",
                "        raise Exception('Not directory!')\n",
                "    s = path_name.split('_')\n",
                "    dc = {'date': s[0], 'protocol': s[1], 'analyst': s[2], 'gn': s[3]}\n",
                "    return dc\n",
                "\n",
                "\n",
                "def make_base_name(date, gn):\n",
                "    return date + '_' + gn + '_-_'\n",
                "\n",
                "\n",
                "for work_dir in dirs:\n",
                "    p = parse_dir_name(work_dir)\n",
                "    print(p)\n",
                "    b = make_base_name(p['date'], p['gn'])\n",
                "    print(b)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from os import path\n",
                "\n",
                "\n",
                "def make_input_paths(input_dir):\n",
                "    print(input_dir)\n",
                "    p = parse_dir_name(input_dir)\n",
                "    print(p)\n",
                "    base_name = make_base_name(p['date'], p['gn'])\n",
                "    worklist = path.join(input_dir, base_name + 'worklist-ELISA.xls')\n",
                "    if not path.isfile(worklist):\n",
                "        raise Exception(\"Worklist file path is invlaid: {}\".format(worklist))\n",
                "\n",
                "    params = path.join(input_dir, base_name +\n",
                "                       p['protocol'] + '_Parameters.csv')\n",
                "    if not path.isfile(params):\n",
                "        raise Exception(\"Parameters file path is invlaid: {}\".format(params))\n",
                "\n",
                "    return {'worklist': worklist, 'params': params}\n",
                "\n",
                "\n",
                "make_input_paths(dirs[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parsing / checking worklist and params path\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "\n",
                "def parse_file_path(path_name):\n",
                "    print(path_name)\n",
                "    if not os.path.isfile(path_name):\n",
                "        raise Exception('Not file!')\n",
                "    fl = os.path.split(path_name)\n",
                "    s = fl[1].split('_')\n",
                "    dc = {'dir': fl[0], 'file': fl[1], 'date': s[0],\n",
                "          'gn': s[1], 'analyst': s[2], 'protocol': s[3]}\n",
                "    return dc\n",
                "\n",
                "\n",
                "# params_path = 'c:/work/report-gen/reports/all/230530_AAV9-ELISA_sey_GN004240-040/230530_GN004240-040_-_AAV9-ELISA_Parameters.csv'\n",
                "params_path = './../reports/all/230530_AAV9-ELISA_sey_GN004240-040/230530_GN004240-040_-_AAV9-ELISA_Parameters.csv'\n",
                "# worklist_path = 'c:/work/report-gen/reports/all/230530_AAV9-ELISA_sey_GN004240-040/230530_GN004240-040_-_worklist-ELISA.xls'\n",
                "worklist_path = './../reports/all/230530_AAV9-ELISA_sey_GN004240-040/230530_GN004240-040_-_worklist-ELISA.xls'\n",
                "\n",
                "htp = os.path.split(params_path)\n",
                "print('params path split {} / {}'.format(htp[0], htp[1]))\n",
                "pp = parse_file_path(params_path)\n",
                "print(pp)\n",
                "\n",
                "htw = os.path.split(worklist_path)\n",
                "# print('worklist path split {} / {}'.format(htw[0], htw[1]))\n",
                "pw = parse_file_path(worklist_path)\n",
                "print(pw)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datetime import datetime, date, time, timezone\n",
                "\n",
                "dt = datetime.strptime(\"21/11/06 16:30\", \"%d/%m/%y %H:%M\")\n",
                "dt = datetime.strptime('230530', \"%y%m%d\")\n",
                "print(dt.strftime('%d %b %Y'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from worklist import read_worklist\n",
                "\n",
                "worklist_path = './../reports/231005_AAV9-ELISA_fff_GN004360-090/231005_GN004360-090_-_worklist-ELISA.xls'\n",
                "worklist_predil_path = path.splitext(worklist_path)[0] + '_ManualDil.xlsx'\n",
                "wl = read_worklist(worklist_path)\n",
                "wl_pdil = read_worklist(worklist_predil_path)\n",
                "display(wl)\n",
                "\n",
                "wl['Dilution_1'] = wl['Dilution_1'] * wl_pdil['Dilution_1']\n",
                "wl['Dilution_2'] = wl['Dilution_2'] * wl_pdil['Dilution_2']\n",
                "wl['Dilution_3'] = wl['Dilution_3'] * wl_pdil['Dilution_3']\n",
                "wl['Dilution_4'] = wl['Dilution_4'] * wl_pdil['Dilution_4']\n",
                "wl"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Read params from json"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "from os import path\n",
                "from hamrep.config import REFVAL_NAME, DIL_NAME\n",
                "\n",
                "working_dir = './reports/230426_AAV9-ELISA_igi_GN004240-033'\n",
                "params_path_default = path.join('./data', 'params.json')\n",
                "params_path_local = path.join(working_dir, 'params.json')\n",
                "params_path = None\n",
                "\n",
                "if path.exists(params_path_local):\n",
                "    params_path = params_path_local\n",
                "    print(f'loading local params {params_path}')\n",
                "elif path.exists(params_path_default):\n",
                "    params_path = params_path_default\n",
                "    print(f'loading default params {params_path}')\n",
                "\n",
                "\n",
                "with open(params_path_default) as json_file:\n",
                "    data = json.load(json_file)\n",
                "    dilutions = data[DIL_NAME]\n",
                "    ref_val_max = data[REFVAL_NAME]\n",
                "\n",
                "print(f'{ref_val_max}, {dilutions}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Convert parameters CSV to json"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "scv_filepath = './reports/230426_AAV9-ELISA_igi_GN004240-033/230426_GN004240-033_-_AAV9-ELISA_Parameters.csv'\n",
                "df = pd.read_csv(scv_filepath, sep=';', index_col='Variable', header=0)\n",
                "# df = pd.read_csv(scv_filepath, sep=';', index_col=False)\n",
                "display(df)\n",
                "# json_filepath = path.splitext(scv_filepath)[0] + '.json'\n",
                "# df.to_json(json_filepath, indent=4, orient=\"columns\", force_ascii=False)\n",
                "# df.to_json()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "int(df.loc['IncubationTime_Samples', :].values[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parameters json"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "parameters_path = './reports/230426_AAV9-ELISA_igi_GN004240-033/230426_GN004240-033_-_AAV9-ELISA_Parameters.json'\n",
                "with open(parameters_path) as json_file:\n",
                "    p = json.load(json_file)\n",
                "\n",
                "p"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = {\n",
                "    \"base_url\": \"https://example.com/api\",\n",
                "    \"timeout\": 3,\n",
                "    \"nested\": {\n",
                "        \"aaa\": 0\n",
                "    }\n",
                "}\n",
                "\n",
                "\n",
                "def update_config(**kwargs):\n",
                "    for key, value in kwargs.items():\n",
                "        if key in {\"api_key\", \"base_url\", \"timeout\", \"nested\"}:\n",
                "            config[key] = value\n",
                "        else:\n",
                "            raise KeyError(key)\n",
                "\n",
                "\n",
                "update_config(nested={\"b\": 1})\n",
                "config"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parsing original data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import chardet\n",
                "from pathlib import Path\n",
                "\n",
                "orig_file_name = 'C:/work/report-gen/reports/export/230712_AAV8-ELISA_sey_GN004240-048/230712_AAV8-ELISA__1_20230712_090748.txt'\n",
                "\n",
                "\n",
                "def get_encoding(file_name):\n",
                "    blob = Path(file_name).read_bytes()\n",
                "    result = chardet.detect(blob)\n",
                "    charenc = result['encoding']\n",
                "\n",
                "    return charenc\n",
                "\n",
                "\n",
                "charenc = get_encoding(orig_file_name)\n",
                "print(f'encoding is `{charenc}`')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SKIP_LINES = 3\n",
                "SKIP_BEGIN = 1\n",
                "\n",
                "\n",
                "def read_exported_data(file_name):\n",
                "    count = 0\n",
                "    csv_str = ''\n",
                "    char_enc = get_encoding(file_name)\n",
                "    with open(file_name, encoding=char_enc) as fp:\n",
                "        for line in fp:\n",
                "            count += 1\n",
                "            if count < SKIP_LINES:\n",
                "                continue\n",
                "            sline = line.rstrip('\\n')\n",
                "            cline = sline.replace('\\t', ',')\n",
                "            cline = cline.rstrip(',')\n",
                "            cline = cline[SKIP_BEGIN:]\n",
                "            csv_str += cline + '\\n'\n",
                "            if count == 12:\n",
                "                break\n",
                "    return csv_str\n",
                "\n",
                "\n",
                "expdata = read_exported_data(orig_file_name)\n",
                "print(expdata)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from io import StringIO\n",
                "import pandas as pd\n",
                "from readdata import get_data_crop\n",
                "\n",
                "\n",
                "def read_data_txt(file_path):\n",
                "    strdata = read_exported_data(file_path)\n",
                "    csv_io = StringIO(strdata)\n",
                "    df = pd.read_csv(csv_io, sep=\",\")\n",
                "    # TODO: move ranges to config file\n",
                "    df_450 = get_data_crop(df, range(0, 8), range(1, 13))\n",
                "    df_630 = get_data_crop(df, range(0, 8), range(14, 26))\n",
                "\n",
                "    return df_450, df_630\n",
                "\n",
                "\n",
                "df450, df630 = read_data_txt(orig_file_name)\n",
                "display(df450)\n",
                "display(df630)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parse photometer exported file name"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from datetime import datetime\n",
                "\n",
                "\n",
                "def parse_photometer_filename(path_name):\n",
                "    if not os.path.isfile(path_name):\n",
                "        raise Exception('Not a directory!')\n",
                "    fle = os.path.split(path_name)[1]\n",
                "    fl = os.path.splitext(fle)\n",
                "    s = fl[0].split('_')\n",
                "    dt = datetime.strptime(s[4]+s[5], \"%Y%m%d%H%M%S\")\n",
                "    dc = {'datetime': dt, 'plate': s[3], 'protocol': s[1]}\n",
                "    return dc\n",
                "\n",
                "\n",
                "photom_path = './reports/export/230712_AAV8-ELISA_sey_GN004240-048/230712_AAV8-ELISA__1_20230712_090748.txt'\n",
                "\n",
                "pp = parse_photometer_filename(photom_path)\n",
                "print(pp)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dt = pp['datetime']\n",
                "dt.strftime('%Y%m%d_%H%M%S')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dt.strftime('%y%m%d')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.path.splitext(photom_path)[1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from mkinout import parse_dir_name\n",
                "\n",
                "p = parse_file_path(photom_path)\n",
                "\n",
                "pd = parse_dir_name(p['dir'])\n",
                "print(pd)\n",
                "l = os.listdir(p['dir'])\n",
                "l[1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "rs = r'^{}_{}_.*\\.txt$'.format(pd['date'], pd['protocol'])\n",
                "print(rs)\n",
                "r = re.compile(rs)\n",
                "ll = []\n",
                "for s in l:\n",
                "    m = r.match(s)\n",
                "    if m:\n",
                "        ll.append(s)\n",
                "\n",
                "print(ll)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "\n",
                "def find_analysis(work_dir):\n",
                "    files = os.listdir(work_dir)\n",
                "    rs = r'^{}_{}_.*\\.txt$'.format(pd['date'], pd['protocol'])\n",
                "    r = re.compile(rs)\n",
                "    ll = []\n",
                "    for fl in files:\n",
                "        m = r.match(fl)\n",
                "        if m:\n",
                "            ll.append(fl)\n",
                "    return ll\n",
                "\n",
                "\n",
                "alist = find_analysis(p['dir'])\n",
                "alist"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
