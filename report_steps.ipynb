{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VzMkIHAq87Dq"
      },
      "source": [
        "# Generate report  - POC\n",
        "\n",
        "Automatic report generation from Hamilton measurements.  \n",
        "\n",
        "Fast iteration in an agile way.  \n",
        "Generic approach - different plates setup, prameters, ... all with the same code, no changes needed.  \n",
        "\n",
        "**Python** programming language.  \n",
        "**jupyter** notebook is currently used, with some functions divided into small modules.  \n",
        "**Visual Studio Code** IDE (Integrated Development Environment).  \n",
        "**Markdown** (*.md) format for generated report (Simple, humanly redable).  \n",
        "\n",
        "**Input**:\n",
        " - Worklist file path (*.xls) as used for Hamilton input.\n",
        "   - Sample name\n",
        "   - Dilution\n",
        "   - Viscosity\n",
        " - Measurement results file path (*.xls) as output from Hamilton.\n",
        " - Parameters; constants in code (file path *.json)\n",
        "   - CV (Coefficient of variation) threshold\n",
        "   - Referennce value (1.7954e+10 cp/ml)\n",
        "   - Dilutions [1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]\n",
        "   - Decimal digits for output\n",
        "\n",
        "**Output**:\n",
        "  - Report (*.md, printable to pdf)\n",
        "    - Could be manually edited\n",
        "    - Image files\n",
        "    - Result sheets\n",
        "  - Estimated size <2kB (current)\n",
        "\n",
        "**TODO**:\n",
        "  - Invalid sample:\n",
        "    - CV >THRESHOLD\n",
        "    - Only one point\n",
        "  - Running modes\n",
        "    - Python script - automatic run (command line with parameters)\n",
        "    - GUI; use modules to crete an App (code remains the same, but used from GUI)\n",
        "  - Finalize the report\n",
        "  - Parameters file (*.json)\n",
        "  - Multiple plates (in worklist file)\n",
        "  - Tests (unit, integration)\n",
        "  - Modules\n",
        "  - Extensive testing...\n",
        "  - Automatic print to *.pdf ?\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zV2iKp5f9-Ui"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DScHnqGC95-6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "from os import path\n",
        "import os\n",
        "from decimal import Decimal\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_input_paths(base_path, sample_num):\n",
        "    input_dir = os.path.dirname(os.path.abspath(base_path))\n",
        "    base_name = os.path.basename(os.path.abspath(base_path))\n",
        "    # print(base_name)\n",
        "    # print(base_path, input_dir)\n",
        "    worklist = base_path + 'worklist-ELISA.xls'\n",
        "    if not os.path.isfile(worklist):\n",
        "        raise Exception(\"Worklist file path is invlaid: {}\".format(worklist))\n",
        "    results = base_path + 'calc{}.xlsx'.format(sample_num)\n",
        "    if not os.path.isfile(results):\n",
        "        raise Exception(\"Rewsults file path is invlaid: {}\".format(results))\n",
        "    \n",
        "    report = path.join(input_dir, 'results_{}'.format(sample_num))\n",
        "    report = path.join(report, '{}report_{}.md'.format(base_name, sample_num))\n",
        "\n",
        "    return {'worklist': worklist, 'results': results, 'report': report}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "9QyeTtP3gQN0",
        "outputId": "f07cb86c-f322-432b-abb5-f785f8efd4be"
      },
      "outputs": [],
      "source": [
        "WORKLIST_BASE_PATH = './data/input/230426_GN004240-033_-_'\n",
        "PLATE_ID = 1 # plate id\n",
        "\n",
        "input_files = make_input_paths(WORKLIST_BASE_PATH, 1)\n",
        "WORKLIST_FILE_PATH = input_files['worklist']\n",
        "RESULT_FILE_PATH = input_files['results']\n",
        "REPORT_FILE_PATH = input_files['report']\n",
        "REPORT_DIR = os.path.dirname(os.path.abspath(REPORT_FILE_PATH))\n",
        "\n",
        "data = pd.read_excel(RESULT_FILE_PATH, sheet_name=None)\n",
        "data[\"Result_Overview\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnlk_5U1hPFw"
      },
      "outputs": [],
      "source": [
        "data[\"Data\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "PSYqGcvayoWf",
        "outputId": "810194c2-1da6-42d1-da31-9492b1b3a256"
      },
      "outputs": [],
      "source": [
        "df = data[\"Data\"]\n",
        "df_450 = df.iloc[2:10, 2:14].copy().reset_index(drop=True)\n",
        "df_450.columns = range(df_450.columns.size)\n",
        "df_450"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "ZaJX1s670kFr",
        "outputId": "bd187db0-fd28-4a14-fb8b-df3a921cb45c"
      },
      "outputs": [],
      "source": [
        "df_630 = df.iloc[2:10, 15:28].copy().reset_index(drop=True)\n",
        "df_630.columns = range(df_630.columns.size)\n",
        "df_630"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLpM9pEe4FE5"
      },
      "outputs": [],
      "source": [
        "def get_data_crop(df, row_span, col_span):\n",
        "  crop = df.iloc[row_span, col_span].copy()\n",
        "  crop.reset_index(drop=True, inplace=True)\n",
        "  crop.set_index([['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']], inplace=True)\n",
        "  crop.columns = range(1, crop.columns.size+1)\n",
        "  return crop\n",
        "\n",
        "def read_data_xls(file_path):\n",
        "  data = pd.read_excel(file_path, sheet_name=None)\n",
        "  df_450 = get_data_crop(data[\"Data\"], range(2, 10), range(2, 14))\n",
        "  df_630 = get_data_crop(data[\"Data\"], range(2, 10), range(15, 27))\n",
        "\n",
        "  return df_450, df_630\n",
        "\n",
        "\n",
        "df_450, df_630 = read_data_xls(RESULT_FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "j6EPPrxZ8QxR",
        "outputId": "282a67a2-8c68-4773-aeb3-e4e390ccd694"
      },
      "outputs": [],
      "source": [
        "df_450"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Jy_b0hVf8Sy1",
        "outputId": "e2a976ea-7f4a-4a9d-d7bc-9b1e9a869ab7"
      },
      "outputs": [],
      "source": [
        "df_630"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "vnu4978hHu2d",
        "outputId": "145843c5-16bd-40c8-8226-4cb708b224f1"
      },
      "outputs": [],
      "source": [
        "df_delta = df_450 - df_630\n",
        "df_delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soWRZN7r9tUZ"
      },
      "outputs": [],
      "source": [
        "def to_multi_index(df_single_index, name):\n",
        "  df_multi_idx = df_single_index.stack().to_frame()\n",
        "  df_multi_idx.columns = [name]\n",
        "\n",
        "  return df_multi_idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPJ6L37j_pNp"
      },
      "outputs": [],
      "source": [
        "def read_concat_data(data_file_path):\n",
        "  df_450, df_630 = read_data_xls(data_file_path)\n",
        "  df_delta = df_450 - df_630\n",
        "  df_delta_all = to_multi_index(df_delta, \"OD_delta\")\n",
        "  df_450_all = to_multi_index(df_450, \"OD_450\")\n",
        "  df_630_all = to_multi_index(df_630, \"OD_630\")\n",
        "\n",
        "  return pd.merge(df_delta_all, \n",
        "                  pd.merge(df_450_all, df_630_all, \n",
        "                  left_index=True, right_index=True),\n",
        "                  left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "xoHKHdi6pVOp",
        "outputId": "25c35044-7f6c-405a-e516-cfd8c631a2cb"
      },
      "outputs": [],
      "source": [
        "m = read_concat_data(RESULT_FILE_PATH)\n",
        "m"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLvIjir8ygz"
      },
      "source": [
        "### Layouts\n",
        "\n",
        "First read layouts as arrays and re-arage to dataframe, using plate layout indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGccq5st0L_7"
      },
      "outputs": [],
      "source": [
        "def to_matrix(l, n):\n",
        "  return [l[i:i+n] for i in range(0, len(l), n)]\n",
        "\n",
        "def index_plate_layout(plate_layout):\n",
        "  plate_layout.set_index([['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']], inplace=True)\n",
        "  plate_layout.columns = range(1, plate_layout.columns.size + 1)\n",
        "\n",
        "  return plate_layout\n",
        "\n",
        "def to_plate_layout(lst):\n",
        "  l_2d = to_matrix(lst, 8)\n",
        "  plate_layout = pd.DataFrame(l_2d).T\n",
        "  \n",
        "  return index_plate_layout(plate_layout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "xKUb6ELEiyhD",
        "outputId": "59d43667-88d3-4d57-f97e-e89719a90fb6"
      },
      "outputs": [],
      "source": [
        "import data.layouts as layouts\n",
        "to_plate_layout(layouts.l_plate_layout)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save layouts to CSV, so that ib the future a layout could be defined in i.e. Excel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpkQxIn7bGA0"
      },
      "outputs": [],
      "source": [
        "def save_plate_layout_csv(layout_list, out_file):\n",
        "  l = to_plate_layout(layout_list)\n",
        "  l.to_csv(out_file, index=False)\n",
        "\n",
        "SAVE_LAYOUTS_CSV = False\n",
        "if SAVE_LAYOUTS_CSV:\n",
        "  save_plate_layout_csv(layouts.l_plate_layout, './data/plate_layout.csv')\n",
        "  save_plate_layout_csv(layouts.l_plate_layout_ident, './data/plate_layout_ident.csv')\n",
        "  save_plate_layout_csv(layouts.l_plate_layout_num, './data/plate_layout_num.csv')\n",
        "  save_plate_layout_csv(layouts.l_plate_layout_dil_id, './data/plate_layout_dil_id.csv')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read CSV layout for check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "EhcUq4gagUvF",
        "outputId": "419c96ca-d9f1-4c13-f3f8-ad9e23662719"
      },
      "outputs": [],
      "source": [
        "def read_plate_layout(file_path):\n",
        "  plate_layout = pd.read_csv(file_path)\n",
        "  index_plate_layout(plate_layout)\n",
        "\n",
        "  return plate_layout\n",
        "\n",
        "plate_layout = read_plate_layout('./data/plate_layout.csv')\n",
        "plate_layout_id = read_plate_layout('./data/plate_layout_ident.csv')\n",
        "plate_layout_num = read_plate_layout('./data/plate_layout_num.csv')\n",
        "plate_layout_dil_id = read_plate_layout('./data/plate_layout_dil_id.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(plate_layout_id)\n",
        "display(plate_layout_num)\n",
        "display(plate_layout_dil_id)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine read data from XLSX with layouts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def concat_data_and_layout(df_data, df_layout):\n",
        "  return pd.merge(df_data, df_layout,\n",
        "                  left_index=True, right_index=True)\n",
        "\n",
        "# df_all = concat_data_and_layout(m, to_multi_index(plate_layout, 'plate_layout'))\n",
        "df_all = concat_data_and_layout(m, to_multi_index(plate_layout_id, 'plate_layout_ident'))\n",
        "df_all = concat_data_and_layout(df_all, to_multi_index(plate_layout_num, 'plate_layout_num'))\n",
        "df_all = concat_data_and_layout(df_all, to_multi_index(plate_layout_dil_id, 'plate_layout_dil_id'))\n",
        "display(df_all)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Filter data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df_all.loc[(df_all['plate_layout_ident']=='r')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sample(dfa, type, sample_num):\n",
        "    # TODO: check for valid `type` `and sample_num`\n",
        "    dfa = dfa.loc[(dfa['plate_layout_ident']==type) & (dfa['plate_layout_num']==sample_num)]\n",
        "    return dfa\n",
        "\n",
        "# type ['k', 'b', 's']\n",
        "# sample_num [1:21, 'b']\n",
        "display(get_sample(df_all, 's', 1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dilution to Concentration\n",
        "\n",
        "Define dilution dataframe. The dataframe is indexed according plate layout, index of refference dataframe corresponds to refference of the `plate_layout_dil`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: read reference value from parameters\n",
        "REF_VAL_MAX = 1.7954e+10\n",
        "DILUTIONS = [1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]\n",
        "\n",
        "def make_concentration(ref_val_max, dilution):\n",
        "    conc  = pd.DataFrame({'dilution': dilution})\n",
        "    conc.loc[:, ['concentration']] = conc.apply(lambda x: ref_val_max / x['dilution'], axis=1)\n",
        "    conc.index = range(1, len(dilution) + 1)\n",
        "    return conc\n",
        "\n",
        "reference_conc = make_concentration(REF_VAL_MAX, DILUTIONS)\n",
        "display(reference_conc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the `reference_set_conc` indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(reference_conc.loc[1])\n",
        "print(reference_conc.loc[7])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fitdata\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from scipy.optimize import OptimizeWarning\n",
        "\n",
        "warnings.simplefilter('ignore', RuntimeWarning)\n",
        "warnings.simplefilter('ignore', OptimizeWarning)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get the fitting data from dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ref = df_all.loc[(df_all['plate_layout_ident']=='r')].copy()\n",
        "ref['plate_layout_dil'] = ref['plate_layout_dil_id'].map(reference_conc['concentration'])\n",
        "display(ref)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit with confidence interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIDENCE_INTERVAL = 95.0 # 95% confidence interval = 100*(1-alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import distributions\n",
        "\n",
        "def fit_image(x, y, popt, pcov, file_path, confidence_interval=CONFIDENCE_INTERVAL,\n",
        "    confidence=None, interval_ratio=2.0,\n",
        "    rm_index=[],\n",
        "    sx=None, sy=None, mask_index=[], sna_idx=[],\n",
        "    verbose=False, valid_sample=True, show=True):\n",
        "    r\"\"\"Plot the fitted function with confidence intervals.\n",
        "\n",
        "    Confidence intervals coud be set using `confidence` parameter.\n",
        "    'student-t' method is a correct one producing wider confidence intervals.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : array_like\n",
        "        x-axis reference values.\n",
        "    y : array_like\n",
        "        y-axis reference values.\n",
        "    popt : iterable\n",
        "        Fit parameters.\n",
        "    pcov : \n",
        "        Covariance matrix from fitting algorithm.\n",
        "    file_path : string\n",
        "        Path where the graph image is saved or `None`\n",
        "    confidence_interval : float \n",
        "        Confidence interval in %\n",
        "    interval_ratio: float\n",
        "        Ration of min and max extention of x axis for fitted curve plot.\n",
        "    rm_index: list\n",
        "        Index of a removed point.\n",
        "    sx : array_like\n",
        "        Sample x-values.\n",
        "    sy : array_like\n",
        "        Sample x-values.\n",
        "    mask_index : array_like\n",
        "        Indices of masked samples.\n",
        "    verbose : bool\n",
        "        Print verbose output.\n",
        "    show : bool\n",
        "        Show the graph. If `False` image is saved if file path is given. \n",
        "    \"\"\"\n",
        "\n",
        "    # confidence [None, 'student-t', 'sqrt_err'] \n",
        "    if verbose:\n",
        "        print('parameter', popt)\n",
        "    perr = np.sqrt(np.diag(pcov))\n",
        "    sigma_err = 1.0\n",
        "    chisq = np.sum((perr / sigma_err) ** 2)\n",
        "    if verbose:\n",
        "        print('chisq={0:.4}; error={1}'.format(np.sqrt(chisq), perr))\n",
        "        # print('function calls', infodict['nfev'])\n",
        "\n",
        "    kwargs = { 'marker': 'x'}\n",
        "    if not valid_sample:\n",
        "        kwargs = { 'marker': 'o', 'facecolors':'none'}\n",
        "    if (sx is not None) and (sy is not None):\n",
        "        if len(sx.drop(mask_index, axis=0)) != 0:\n",
        "            plt.scatter(sx.drop(mask_index, axis=0), sy.drop(mask_index, axis=0),\n",
        "                        s=48, linewidths=0.6, label='sample valid', color='forestgreen', **kwargs)\n",
        "        if (len(sx.iloc[mask_index]) != 0) and (list(mask_index) != list(sna_idx)):\n",
        "            plt.scatter(sx.iloc[mask_index], sy.iloc[mask_index],\n",
        "                        s=48, linewidths=0.8, label='sample masked', color='r', **kwargs)\n",
        "\n",
        "    if len(x.drop(rm_index, axis=0)) != 0:\n",
        "        plt.scatter(x.drop(rm_index, axis=0), y.drop(rm_index, axis=0), marker='+',\n",
        "                color='royalblue', s=48, linewidths=0.8, label='reference')\n",
        "    if len(x.iloc[rm_index]) != 0:\n",
        "        plt.scatter(x.iloc[rm_index], y.iloc[rm_index], marker='.',\n",
        "                color='r', s=48, linewidths=0.8, label='reference masked')\n",
        "    plt.xscale('log')\n",
        "    \n",
        "    alpha = (100.0 - confidence_interval) / 100.0 \n",
        "    n = len(y)    # number of data points\n",
        "    p = len(popt) # number of parameters\n",
        "    dof = max(0, n - p) # number of degrees of freedom\n",
        "\n",
        "    # student-t value for the dof and confidence level\n",
        "    tval = distributions.t.ppf(1.0 - alpha / 2., dof) \n",
        "    sigma_popt = np.empty(len(popt), dtype=np.float64)\n",
        "    param_names = ['a', 'b', 'c', 'd']\n",
        "    for i, p, var, pname in zip(range(n), popt, np.diag(pcov), param_names):\n",
        "        sigma = var ** 0.5\n",
        "        st = sigma * tval\n",
        "        sigma_popt[i] = st\n",
        "        if verbose:\n",
        "            print('{0}: {1:.3} [{2:.3}, {3:.3}]; err={4:.3}[{5:.2f}%]'.format(pname, p, p - st, p + st, st, 100*st/p))\n",
        "\n",
        "    if confidence=='None' or confidence=='student-t':\n",
        "        if verbose: print('student-t is used for error estimation using {} degrees of freedom'.format(dof))\n",
        "        popt_high = popt + sigma_popt\n",
        "        popt_low = popt - sigma_popt\n",
        "    else:\n",
        "        if verbose: print('sqrt of covariance matrix diagonal is used for error estimation')\n",
        "        popt_high = popt + perr\n",
        "        popt_low = popt - perr\n",
        "\n",
        "    num_pts = 100\n",
        "    x_min = x.min()\n",
        "    x_max = x.max()\n",
        "    t = np.arange(x_min, x_max, (x_max - x_min) / num_pts)\n",
        "    plt.plot(t, fitdata.func(t, *popt), color='slategray', linewidth=0.2)\n",
        "\n",
        "    sx_n = sx[~sx.isna()] if sx is not None else None\n",
        "    x_min_ext = x.min() / interval_ratio\n",
        "    if sx is not None: x_min_ext = min(x_min_ext, sx_n.min())\n",
        "    if x_min != x_min_ext:\n",
        "        t = np.arange(x_min_ext, x_min, (x_min - x_min_ext) / (num_pts / 10.0))\n",
        "        plt.plot(t, fitdata.func(t, *popt), color='red', linestyle=(0, (5, 10)), linewidth=0.2, label='ext')\n",
        "\n",
        "    x_max_ext = x.max() * interval_ratio\n",
        "    if sx is not None: x_max_ext = max(x_max_ext, sx_n.max())\n",
        "    if x_max != x_max_ext:\n",
        "        t = np.arange(x_max, x_max_ext, (x_max_ext - x_max) / (num_pts / 10.0))\n",
        "        # no label, only one extension labe\n",
        "        plt.plot(t, fitdata.func(t, *popt), color='red', linestyle=(0, (5, 10)), linewidth=0.2)\n",
        "\n",
        "    # show NaN concentration values somewhere -> show OD\n",
        "    sx_na = sx[sx.isna()] if sx is not None else None\n",
        "    if sx is not None:\n",
        "        if verbose: print(\"NaN indices:\", sx_na.index)\n",
        "        sna_idx = sx[~sx.isna()].index\n",
        "        if len(sx_na) != 0:\n",
        "            x_na = np.full(shape=len(sx_na), fill_value=x_min_ext * 0.5)\n",
        "            y_na = sy.drop(sna_idx, axis=0)\n",
        "            plt.scatter(x_na, y_na, marker='_', color='red', s=48, linewidths=0.8, label='backfit failed')\n",
        "\n",
        "    plt.xlabel('concentration [cp/ml]')\n",
        "    plt.ylabel('Optical density')\n",
        "\n",
        "    bound_upper = fitdata.func(t, *popt_high)\n",
        "    bound_lower = fitdata.func(t, *popt_low)\n",
        "    # plotting the confidence intervals\n",
        "    plt.fill_between(t, bound_lower, bound_upper,\n",
        "                    color = 'black', alpha = 0.15)\n",
        "    \n",
        "    plt.legend()\n",
        "\n",
        "    if file_path is not None:\n",
        "         plt.savefig(file_path)\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.clf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_sheet(popt, pcov, n, confidence_interval=95.0):\n",
        "    # `confidence_interval` 95% confidence interval = 100*(1-alpha)\n",
        "    alpha = (100.0 - confidence_interval) / 100.0 \n",
        "    p = len(popt) # number of parameters\n",
        "    dof = max(0, n - p) # number of degrees of freedom\n",
        "\n",
        "    # student-t value for the dof and confidence level\n",
        "    tval = distributions.t.ppf(1.0 - alpha / 2.0, dof) \n",
        "\n",
        "    sigma_popt = np.empty(len(popt), dtype=np.float64)\n",
        "    confidence_interval = [None] * 4\n",
        "    for i, p, var in zip(range(n), popt, np.diag(pcov)):\n",
        "        sigma = var ** 0.5\n",
        "        st = sigma * tval\n",
        "        sigma_popt[i] = st\n",
        "        confidence_interval[i] = '[{0:.3}, {1:.3}]'.format(p - st, p + st)\n",
        "\n",
        "    param_names = ['a', 'b', 'c', 'd']\n",
        "    perr = np.sqrt(np.diag(pcov))\n",
        "\n",
        "    return pd.DataFrame({'Parameter name': param_names, 'Estimated value':popt,\n",
        "        'Error': perr, 'Confidence interval': confidence_interval})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Backfit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def backfit(df, param):\n",
        "    bf = df[['OD_delta', 'plate_layout_dil']].copy()\n",
        "    bf = bf.reindex(['plate_layout_dil', 'OD_delta'], axis=1)\n",
        "    bf.rename(columns={'plate_layout_dil': 'Standard Value [cp/ml]', 'OD_delta': 'Optical density'}, inplace=True)\n",
        "    bf.loc[:, ['Concentration backfit [cp/ml]']] = bf.apply(lambda x: fitdata.inv_func(x['Optical density'], *param), axis=1)\n",
        "    bf.loc[:, ['SV to OD fit']] = bf.apply(lambda x: fitdata.func(x['Standard Value [cp/ml]'], *param), axis=1)\n",
        "    bf.index.name = 'Well'\n",
        "    bf = bf.reindex(['Standard Value [cp/ml]', 'Concentration backfit [cp/ml]', 'Optical density', 'SV to OD fit'], axis=1)\n",
        "    bf.loc[:, ['Recovery rate [%]']] = bf.apply(lambda x: (x['Concentration backfit [cp/ml]'] / x['Standard Value [cp/ml]']) * 100.0, axis=1)\n",
        "    \n",
        "    return bf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_reference_auto_rm(x, y, err_threshold=0.998, verbose=False):\n",
        "    \"\"\"Fits the reference and removes a point to fing min error\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : array_like\n",
        "        x-axis values.\n",
        "    y : array_like\n",
        "        y-axis value.\n",
        "    err_threshold : Eroor threshold to to skip point removal\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    array\n",
        "        parameters of the filt\n",
        "    int\n",
        "        index of the removed point, -1 if no point is removed\n",
        "    float64\n",
        "        error\n",
        "    \"\"\"\n",
        "    fc = fitdata.fit_reference(fitdata.func, x, y)\n",
        "    bfn = lambda l: fitdata.inv_func(l, *fc[0])\n",
        "    x_hat = bfn(y)\n",
        "    r2_max = r2_score(x, x_hat)\n",
        "    idx = []\n",
        "    fit_stats = pd.DataFrame(columns=['idx', 'metric', 'note'])\n",
        "    if verbose:\n",
        "        print('R-squared is invalid for nonlinear models!')\n",
        "        print('metric, index')\n",
        "        print('{0:.5f}, {1}'.format(r2_max, idx))\n",
        "    \n",
        "    if r2_max > err_threshold:\n",
        "        fit_stats.loc[len(fit_stats)] = [-1, r2_max, '']\n",
        "        return fc, idx, r2_max\n",
        "    fit_stats.loc[len(fit_stats)] = [-1, r2_max, 'metric < threshold ({:.4f} < {:.4f})'.format(r2_max, err_threshold)]\n",
        "\n",
        "    for i in range(len(x)):\n",
        "        xd = x.drop([i], axis=0)\n",
        "        yd = y.drop([i], axis=0)\n",
        "        try:\n",
        "            fc_i = fitdata.fit_reference(fitdata.func, xd.to_numpy(), yd.to_numpy())\n",
        "        except Exception as e:\n",
        "            estr = '{0}, {1} Reason: {2}'.format(np.nan, [i], str(e))\n",
        "            if verbose: print(e)\n",
        "            fit_stats.loc[len(fit_stats)] = [i, np.nan, str(e)]\n",
        "            continue\n",
        "\n",
        "        bfn = lambda l: fitdata.inv_func(l, *fc_i[0])\n",
        "        x_hat = bfn(yd)\n",
        "        r_squared = r2_score(xd, x_hat)\n",
        "        if np.isinf(fc_i[1]).any():\n",
        "            fit_stats.loc[len(fit_stats)] = [i, r_squared, 'Invalid covariance matrix.']\n",
        "            continue\n",
        "\n",
        "        fit_stats.loc[len(fit_stats)] = [i, r_squared, '']\n",
        "        if verbose: print('{0:.4f}, {1}'.format(r_squared, i))\n",
        "        if (r_squared > r2_max) and not np.isinf(fc_i[1]).any():\n",
        "            r2_max = r_squared\n",
        "            fc = fc_i\n",
        "            idx = [i]\n",
        "\n",
        "    fit_stats.set_index('idx', inplace=True)\n",
        "    fit_stats.loc[idx, 'note'] = 'Maximum.'\n",
        "    \n",
        "    return fc, idx, r2_max, fit_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = ref.reset_index(level=[0,1])['plate_layout_dil']\n",
        "y = ref.reset_index(level=[0,1])['OD_delta']\n",
        "fit_result = fit_reference_auto_rm(x, y, verbose=False)\n",
        "popt = fit_result[0][0]\n",
        "pcov = fit_result[0][1]\n",
        "\n",
        "fit_image(x, y, fit_result[0][0], fit_result[0][1], None, confidence='student-t', rm_index=fit_result[1])\n",
        "display(fit_result[3])\n",
        "display(fit_result[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bf = backfit(ref, popt)\n",
        "display(bf)\n",
        "\n",
        "od_min = bf['Optical density'].min()\n",
        "od_max = bf['Optical density'].max()\n",
        "print('Optical density range <{0:.4f}, {1:.4f}>'.format(od_min, od_max))\n",
        "\n",
        "od_fit_min = bf['SV to OD fit'].min()\n",
        "od_fit_max = bf['SV to OD fit'].max()\n",
        "print('SV to OD fit range <{0:.4f}, {1:.4f}>'.format(od_fit_min, od_fit_max))\n",
        "\n",
        "sv_min = bf['Standard Value [cp/ml]'].min()\n",
        "sv_max = bf['Standard Value [cp/ml]'].max()\n",
        "print('Standard Value [cp/ml] range <{0}, {1}>'.format(sv_min, sv_max))\n",
        "\n",
        "cb_min = bf['Concentration backfit [cp/ml]'].min()\n",
        "cb_max = bf['Concentration backfit [cp/ml]'].max()\n",
        "print('Concentration backfit [cp/ml] range <{0}, {1}>'.format(cb_min, cb_min))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(get_sample(df_all, 's', 1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the data, and apply the inverse function as a check..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "samplesk = df_all.loc[(df_all['plate_layout_ident']=='s') | (df_all['plate_layout_ident']=='k') | (df_all['plate_layout_ident']=='r')]\n",
        "samplesk.loc[:, ['plate_layout_dil']] = samplesk['plate_layout_dil_id'].map(reference_conc['dilution'])\n",
        "display(samplesk)\n",
        "\n",
        "def unique_sample_numbers(df):\n",
        "  sample_nums = df['plate_layout_num'].astype(int).unique()\n",
        "  sample_nums.sort()\n",
        "  return sample_nums\n",
        "\n",
        "sample_nums = unique_sample_numbers(samplesk)\n",
        "display(sample_nums)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For Coefficient of variation compution we need to use ddof (degrees of freedom) parameter set to `ddof=1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import variation\n",
        "import numpy\n",
        "\n",
        "def conc_func(x, dil, *popt):\n",
        "    return fitdata.inv_func(x, *popt) * dil\n",
        "\n",
        "\n",
        "# process sample\n",
        "sample_num_t = 11\n",
        "sample_type_t = 's'\n",
        "a_sample = get_sample(samplesk, sample_type_t, sample_num_t)\n",
        "a_sample.loc[:, ['concentration']] = a_sample.apply(lambda x: conc_func(x['OD_delta'], x['plate_layout_dil'], *popt), axis=1)\n",
        "display(a_sample)\n",
        "\n",
        "smp_t = a_sample[~a_sample.concentration.isna()]\n",
        "display(smp_t)\n",
        "test_cv = variation(smp_t['concentration'], ddof=1)\n",
        "test_mean = numpy.mean(smp_t['concentration'])\n",
        "print(\"CV %f\" % test_cv)\n",
        "print(\"mean %f\" % test_mean)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute concentration for all `s` and `k` samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "samplesk.loc[:, ['concentration']] = samplesk.apply(lambda x: conc_func(x['OD_delta'], x['plate_layout_dil'], *popt), axis=1)\n",
        "samplesk.loc[:, ['backfit']] = samplesk.apply(lambda x: fitdata.inv_func(x['OD_delta'], *popt), axis=1)\n",
        "samplesk"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mask_reason_fn(val, odmin, odmax, note):\n",
        "    if val < odmin:\n",
        "        return '{2} {0:.3e} < {1:.3e}'.format(Decimal(val), Decimal(odmin), note)\n",
        "    if val > odmax:\n",
        "        return '{2} {0:.3e} > {1:.3e}'.format(Decimal(val), Decimal(odmin), note)\n",
        "    if math.isnan(val):\n",
        "        return 'NaN'\n",
        "    return None\n",
        "\n",
        "def mask_reason_short_fn(val, vmin, vmax, dil, note):\n",
        "    if val < vmin:\n",
        "        return '<{:.3e}'.format(Decimal(vmin * dil))\n",
        "    if val > vmax:\n",
        "        return '>{:.3e}'.format(Decimal(vmax * dil))\n",
        "    if math.isnan(val):\n",
        "        return 'Backfit failed.'\n",
        "    return None\n",
        "\n",
        "print('Optical density range = <{0:.4f}, {1:.4f}>'.format(od_min, od_max))\n",
        "print('Backfit range = <{0:.4e}, {1:.4e}>'.format(Decimal(sv_min), sv_max))\n",
        "od_note = 'Measured OD'\n",
        "samplesk.loc[:, ['od_mask_reason']] = samplesk.apply(lambda x: mask_reason_fn(x['OD_delta'], od_min, od_max, od_note), axis=1)\n",
        "sv_note = 'Backfit'\n",
        "samplesk.loc[:, ['mask_reason']] = samplesk.apply(lambda x: mask_reason_short_fn(x['backfit'], cb_min, cb_max, x['plate_layout_dil'], ''), axis=1)\n",
        "samplesk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAMPLE_TYPES = {'s':'sample', 'k':'controll', 'r':'refference', 'b':'blank'}\n",
        "\n",
        "def process_sample(samples, stype, sample_num):\n",
        "    sample = get_sample(samples, stype, sample_num)\n",
        "    smp_t = sample[sample.mask_reason.isna()]\n",
        "    cv = np.nan\n",
        "    mean = np.nan\n",
        "    if len(smp_t['concentration']) > 1:\n",
        "        cv = variation(smp_t['concentration'], ddof=1)\n",
        "        mean = numpy.mean(smp_t['concentration'])\n",
        "    elif len(smp_t['concentration']) == 1:\n",
        "        mean = numpy.mean(smp_t['concentration'])\n",
        "\n",
        "    return sample, cv, mean\n",
        "\n",
        "\n",
        "CV_THRESHOLD = 0.2 # 20%\n",
        "MIN_VALID_SAMPLE_POINTS = 2\n",
        "\n",
        "def sample_check(samples, stype, sample_num, cv_thresh=CV_THRESHOLD,\n",
        "                 min_valid_pts=MIN_VALID_SAMPLE_POINTS):\n",
        "    s = process_sample(samples, stype, sample_num)\n",
        "    valid = True\n",
        "    note = ''\n",
        "    if s[1] > cv_thresh:\n",
        "        note = 'CV > {}; '.format(cv_thresh)\n",
        "        valid = False\n",
        "    smp = s[0]\n",
        "    valid_pts = smp['mask_reason'].isna().sum()\n",
        "    if valid_pts < min_valid_pts:\n",
        "        note += 'Not enough valid sample points. Required {}, available {};'.format(min_valid_pts, valid_pts)\n",
        "        valid = False\n",
        "    elif valid_pts != len(smp['mask_reason']):\n",
        "        note += 'Reduced number of sample points. Measured {}, valid {};'.format(len(smp['mask_reason']), valid_pts)\n",
        "        valid &= True\n",
        "\n",
        "    note_cols = smp[~smp['mask_reason'].isna()]\n",
        "    if len(note_cols)!= 0:\n",
        "        if (note_cols['mask_reason'] == note_cols['mask_reason'][0]).all():\n",
        "            note += note_cols['mask_reason'][0] + ';' + note_cols['od_mask_reason'][0]\n",
        "        # else:\n",
        "        #     note += note_cols['mask_reason'].str.cat(sep=', ')\n",
        "\n",
        "    return {'sample':smp, 'cv':s[1], 'mean':s[2], 'note':note, 'type':stype, 'num':sample_num, 'valid':valid, 'valid_pts': valid_pts}\n",
        "\n",
        "\n",
        "def print_sample(number, stype, sample, cv, mean):\n",
        "    display(sample[['OD_delta', 'plate_layout_dil', 'concentration', 'backfit', 'mask_reason']])\n",
        "    print(\"{1} '{2}' {0}\".format(number, SAMPLE_TYPES[stype], stype))\n",
        "    print(\"CV = {:2.3} [%]\".format(100 * cv))\n",
        "    print(\"mean = {:.4} [cp/ml]\".format(mean))\n",
        "\n",
        "\n",
        "def print_sample_dc(sample_dict):\n",
        "    display(sample_dict['sample'][['OD_delta', 'plate_layout_dil', 'concentration', 'backfit', 'mask_reason']])\n",
        "    print(\"{1} '{2}' {0}\".format(sample_dict['num'], SAMPLE_TYPES[sample_dict['type']], sample_dict['type']))\n",
        "    print(\"CV = {:2.3} [%]\".format(100 * sample_dict['cv']))\n",
        "    print(\"mean = {:.4} [cp/ml]\".format(sample_dict['mean']))\n",
        "    print(\"valid = {}\".format(sample_dict['valid']))\n",
        "    print(\"note: {}\".format(sample_dict['note']))\n",
        "\n",
        "sc = sample_check(samplesk, 'k', 1)\n",
        "sample_results = pd.DataFrame(columns=['id', 'cv', 'cp_mean', 'Note', 'Valid'])\n",
        "for i in [14]:#sample_nums: [5, 6, 9]\n",
        "    stype = 's'\n",
        "    # s = process_sample(samplesk, 's', i)\n",
        "    # print_sample(i, 's', *s)\n",
        "    sc = sample_check(samplesk, 's', i)\n",
        "    print_sample_dc(sc)\n",
        "    sample_results.loc[len(sample_results)] = ['sample {:02d}'.format(i),\n",
        "                                               sc['cv'], sc['mean'], sc['note'], sc['valid']]\n",
        "    if i == 3: break;\n",
        "\n",
        "display(sample_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "\n",
        "class SampleInfo(str, Enum):\n",
        "    NAN_LOW = 'NaN below reference'\n",
        "    NAN_HIGH = 'NaN above reference'\n",
        "    LOW = 'value below reference'\n",
        "    HIGH = 'value above reference'\n",
        "    CV = 'CV above threshold'\n",
        "    VALID_PTS = 'few valid points'\n",
        "\n",
        "\n",
        "def sampleinfo_to_str(info, multiplier=1.0):\n",
        "    if info is None:\n",
        "        return None\n",
        "\n",
        "    if not info:\n",
        "        return None;\n",
        "    \n",
        "    if info['enum'] == SampleInfo.CV:\n",
        "        return 'CV>{:.1f}%({:.1f}%)'.format(CV_THRESHOLD * 100, float(info['value']) * 100.0)\n",
        "\n",
        "    if info['enum'] == SampleInfo.VALID_PTS:\n",
        "        return '{} valid point'.format(info['value'])\n",
        "\n",
        "    return '{}{:.4e}'.format(info['sign'], float(info['value']) * multiplier)\n",
        "\n",
        "\n",
        "def sample_info(samples, stype, sample_num, verbose=False):\n",
        "    s = get_sample(samples, stype, sample_num)\n",
        "    sc = sample_check(samples, stype, sample_num)\n",
        "    if verbose:\n",
        "        display(s)\n",
        "        # display(k)\n",
        "        print('OD=[{}, {}]'.format(od_min, od_max))\n",
        "        print('OD_fit=[{:.3}, {:.3}]'.format(Decimal(od_fit_min), Decimal(od_fit_max)))\n",
        "        print('SV=[{:.3e}, {:.3e}]'.format(Decimal(sv_min), Decimal(sv_max)))\n",
        "        print('CB=[{}, {}]'.format(cb_min, cb_max))\n",
        "    above_ref_od_max = s['OD_delta'] > od_fit_max\n",
        "    below_ref_od_min = s['OD_delta'] < od_fit_min\n",
        "    msgdc = {}\n",
        "    if s['backfit'].isna().all():\n",
        "        if above_ref_od_max.all():\n",
        "            msgdc = {'sign': '>', 'value': Decimal(sv_max), 'enum': SampleInfo.NAN_HIGH}\n",
        "        if below_ref_od_min.all():\n",
        "            msgdc = {'sign': '<', 'value': Decimal(sv_min), 'enum': SampleInfo.NAN_LOW}\n",
        "    elif sc['cv'] > CV_THRESHOLD:\n",
        "        msgdc = {'sign': '>{:.2f}'.format(CV_THRESHOLD), 'value': sc['cv'], 'enum': SampleInfo.CV}\n",
        "    elif not s['mask_reason'].isna().all():\n",
        "        t = s[['OD_delta', 'plate_layout_dil', 'concentration', 'backfit']]\n",
        "        t_not_na = t[~t['backfit'].isna()]\n",
        "        \n",
        "        if t_not_na['OD_delta'].max() < od_fit_min:\n",
        "            t_below_ref = t_not_na[below_ref_od_min]\n",
        "            # msgdc = {'sign': '<', 'value': t_below_ref['concentration'].max(), 'enum': SampleInfo.LOW}\n",
        "            msgdc = {'sign': '<', 'value': Decimal(sv_min * sc['sample']['plate_layout_dil'].min()), 'enum': SampleInfo.LOW}\n",
        "        elif t_not_na['OD_delta'].min() > od_fit_max:\n",
        "            t_above_ref = t_not_na[above_ref_od_max]\n",
        "            # display(sc['sample'])\n",
        "            print('*** {} *  {} = {}'.format(sv_max, sc['sample']['plate_layout_dil'].max(), sv_max * sc['sample']['plate_layout_dil'].max()))\n",
        "            msgdc = {'sign': '>', 'value': Decimal(sv_max * sc['sample']['plate_layout_dil'].max()), 'enum': SampleInfo.HIGH}\n",
        "    \n",
        "    if sc['valid_pts'] < MIN_VALID_SAMPLE_POINTS and sc['valid_pts'] != 0:\n",
        "        msgdc = {'sign': '', 'value': sc['valid_pts'], 'enum': SampleInfo.VALID_PTS}\n",
        "\n",
        "    del sc['sample']\n",
        "    del sc['note']\n",
        "    sc['info'] = msgdc\n",
        "    return sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "si = sample_info(samplesk, 's', 6)\n",
        "si"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_results = pd.DataFrame(columns=['id', 'CV [%]', 'Reader Data [cp/ml]', 'Note', 'Valid', 'info'])\n",
        "knum = 1\n",
        "s = sample_check(samplesk, 'k', knum)\n",
        "si = sample_info(samplesk, 'k', knum)\n",
        "display(si)\n",
        "sample_results.loc[len(sample_results)] = ['control {:02d}'.format(knum), s['cv'], s['mean'], s['note'], s['valid'], si]\n",
        "\n",
        "rnum = 1\n",
        "s = sample_check(samplesk, 'r', rnum)\n",
        "si = sample_info(samplesk, 'r', knum)\n",
        "sample_results.loc[len(sample_results)] = ['reference {:02d}'.format(knum), s['cv'], s['mean'], s['note'], s['valid'], si]\n",
        "\n",
        "for i in sample_nums:\n",
        "    stype = 's'\n",
        "    s = sample_check(samplesk, 's', i)\n",
        "    si = sample_info(samplesk, 's', i)\n",
        "    sample_results.loc[len(sample_results)] = ['sample {:02d}'.format(i), s['cv'], s['mean'], s['note'], s['valid'], si]\n",
        "\n",
        "sample_results.set_index(sample_results['id'], inplace=True)\n",
        "sample_results = sample_results.drop('id', axis=1)\n",
        "sl = sample_results\n",
        "display(sl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "si = sample_info(samplesk, 's', 6, True)\n",
        "display(si)\n",
        "sampleinfo_to_str(si['info'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot sample with referene curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mask_index(df):\n",
        "    b = df.reset_index(level=[0,1])\n",
        "    b = b[b['mask_reason'].notna()]\n",
        "\n",
        "    return b.index\n",
        "\n",
        "\n",
        "def na_index(df):\n",
        "    b = df.reset_index(level=[0,1])\n",
        "    b = b[b['backfit'].isna()]\n",
        "    \n",
        "    return b.index\n",
        "\n",
        "\n",
        "def sample_img(samples, sample_type, sample_num, img_file=None, show=True, verbose=False):\n",
        "    sd = sample_check(samplesk, sample_type, sample_num)\n",
        "    if verbose:\n",
        "        print(sample_type, sample_num)\n",
        "        display(sd['sample'])\n",
        "\n",
        "    mask_idx = mask_index(sd['sample'])\n",
        "    x = ref.reset_index(level=[0,1])['plate_layout_dil']\n",
        "    y = ref.reset_index(level=[0,1])['OD_delta']\n",
        "    fit_result = fit_reference_auto_rm(x, y, verbose=verbose)\n",
        "    # compute original concenmtration \n",
        "    sd['sample'].loc[:, ['conc_plot']] = sd['sample'].apply(lambda x: x['concentration'] / x['plate_layout_dil'], axis=1)\n",
        "    sx = sd['sample'].reset_index(level=[0,1])['conc_plot']\n",
        "    sy = sd['sample'].reset_index(level=[0,1])['OD_delta']\n",
        "    fit_image(x, y, fit_result[0][0], fit_result[0][1], img_file, confidence='student-t',\n",
        "              rm_index=fit_result[1], mask_index=mask_idx,\n",
        "              sx=sx, sy=sy, sna_idx=na_index(sd['sample']), show=show, valid_sample=sd['valid'], interval_ratio=1.0)\n",
        "    # display(na_index(sd['sample']))\n",
        "    \n",
        "sample_img(samplesk, 's', 6)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Worklist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def worklist_sample(worklist_file, sample_id):\n",
        "    wl = pd.read_excel(worklist_file)\n",
        "    wl.set_index([['control 01', 'reference 01', 'blank', 'sample 01', 'sample 02', 'sample 03',\n",
        "        'sample 04', 'sample 05', 'sample 06', 'sample 07', 'sample 08', 'sample 09', 'sample 10',\n",
        "        'sample 11', 'sample 12', 'sample 13', 'sample 14', 'sample 15', 'sample 16', 'sample 17',\n",
        "        'sample 18', 'sample 19', 'sample 20', 'sample 21']], inplace=True)\n",
        "    wl.drop('blank', axis=0, inplace=True)\n",
        "    wl.index.name = 'Sample type'\n",
        "    cols_id =['SampleID', 'Dilution', 'Viscosity']\n",
        "    cols = [x + '_' + str(sample_id) for x in cols_id]\n",
        "    cols_dict = {x : y for x,y in zip(cols_id, cols)}\n",
        "\n",
        "    invalid_sample = wl['SampleID_{}'.format(sample_id)].isnull().values.any()\n",
        "    if invalid_sample:\n",
        "        return None, None\n",
        "    \n",
        "    return wl[cols], cols_dict\n",
        "\n",
        "wl, wl_cols_dict = worklist_sample(WORKLIST_FILE_PATH, PLATE_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def final_sample_info(all_info, pre_dilution, verbose=False):\n",
        "    # print('INFO', all_info)\n",
        "    info = all_info['info']\n",
        "    if not info:\n",
        "        return ''\n",
        "    \n",
        "    msg = ''\n",
        "    if info['enum'] == SampleInfo.NAN_HIGH:\n",
        "        msg = '>{:.4e}'.format(info['value'] * pre_dilution)\n",
        "        # print(SampleInfo.NAN_HIGH)\n",
        "    elif info['enum'] == SampleInfo.NAN_LOW:\n",
        "        msg = '<{:.4e}'.format(info['value'] * pre_dilution)\n",
        "        # print(SampleInfo.NAN_LOW)\n",
        "    elif info['enum'] == SampleInfo.HIGH:\n",
        "        # msg = '>{:.4e}'.format(info['value'] * pre_dilution)\n",
        "        msg = '>{:.4e}'.format(info['value'] * pre_dilution)\n",
        "        # print(SampleInfo.HIGH, pre_dilution, info['value'])\n",
        "    elif info['enum'] == SampleInfo.LOW:\n",
        "        msg = '<{:.4e}'.format(info['value'] * pre_dilution)\n",
        "        # print(SampleInfo.LOW)\n",
        "    elif info['enum'] == SampleInfo.VALID_PTS:\n",
        "        msg = '{} valid point'.format(all_info['valid_pts'])\n",
        "        # print(SampleInfo.VALID_PTS)\n",
        "    elif info['enum'] == SampleInfo.CV:\n",
        "        msg = 'CV>{:.2f}%({:.2f}%)'.format(CV_THRESHOLD * 100.0, info['value'] * 100.0)\n",
        "    else:\n",
        "        msg = ''\n",
        "\n",
        "    # print(msg)\n",
        "    return msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: nasty, using globals!!!\n",
        "def make_final():\n",
        "    final = pd.concat([wl, sl], axis=1)\n",
        "    cd = wl_cols_dict\n",
        "    final.loc[:, ['Result [cp/ml]']] = final.apply(lambda x: x['Reader Data [cp/ml]'] * x[cd['Dilution']], axis=1)\n",
        "    final.loc[:, ['CV [%]']] = final.apply(lambda x: x['CV [%]'] * 100, axis=1)\n",
        "    # reorder columns\n",
        "    final = final.reindex([cd['SampleID'], cd['Dilution'], cd['Viscosity'], 'Reader Data [cp/ml]', 'Result [cp/ml]', 'CV [%]', 'Valid', 'info'], axis=1)\n",
        "    final.rename(columns={cd['SampleID']: 'Sample Name', cd['Dilution']: 'Pre-dilution'}, inplace=True)\n",
        "    final.drop('Viscosity_1', axis=1, inplace=True)\n",
        "    final.index.name = 'Sample type'\n",
        "    return final\n",
        "\n",
        "final = make_final()\n",
        "final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final = make_final()\n",
        "final.loc[:, ['info_ex']] = final.apply(lambda x: final_sample_info(x['info'], x['Pre-dilution']), axis=1)\n",
        "final"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pbq4Hm3A-DPa"
      },
      "source": [
        "## Plate Layout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = plate_layout_num.replace({'b':-99}).astype(float)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "YHCaH9mqvaPo",
        "outputId": "dbd268eb-cf2b-4789-a9cd-ba72817b9821"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "vals = np.around(df.values, 2)\n",
        "norm = plt.Normalize(vals.min()-1, vals.max()+1)\n",
        "colours = plt.cm.hot(norm(vals))\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "ax = fig.add_subplot(111, frameon=False, xticks=[], yticks=[])\n",
        "\n",
        "the_table=plt.table(cellText=vals, rowLabels=df.index, colLabels=df.columns,\n",
        "                    loc='center', cellColours=colours)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gZvoKky5-zeC"
      },
      "source": [
        "## Report  \n",
        "We build a report here..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit Reference Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_section_md(df_ref, popt, pcov, out_dir):\n",
        "    x = df_ref.reset_index(level=[0,1])['plate_layout_dil']\n",
        "    y = df_ref.reset_index(level=[0,1])['OD_delta']\n",
        "    fit_result = fit_reference_auto_rm(x, y)\n",
        "    result_img = path.join(out_dir, 'fit.png')\n",
        "    fit_image(x, y, fit_result[0][0], fit_result[0][1], result_img, confidence='student-t', rm_index=fit_result[1])\n",
        " \n",
        "    n = len(x) - len(fit_result[1])\n",
        "    df_fit = fit_sheet(popt, pcov, n)\n",
        "    display(df_fit)\n",
        "\n",
        "    md = '## Reference Curve Fit\\n\\n'\n",
        "    md += '$\\LARGE y = {d + {a - d \\over {1 + ({ x \\over c })^b}} }$  \\n\\n'\n",
        "    md += '![\"alt text\"](./img/fit.png)'\n",
        "\n",
        "    md += '\\n\\n'\n",
        "    md += 'Verbose fitting progress, metric is R-squared:\\n\\n'\n",
        "    md += fit_result[3].to_markdown() + '\\n\\n'\n",
        "\n",
        "    md += 'Fit parameters\\n\\n'\n",
        "    md += df_fit.to_markdown(index=False) + '\\n\\n'\n",
        "    md += 'Backfit...'\n",
        "    fit_result = fit_reference_auto_rm(x, y)\n",
        "    df_backfit = backfit(df_ref, fit_result[0][0])\n",
        "    md += '\\n\\n' + df_backfit.to_markdown() + '\\n\\n'\n",
        "\n",
        "    # cv = variation(df_backfit['concentration'], ddof=1)\n",
        "\n",
        "    return md\n",
        "\n",
        "# fit_section_md(ref, popt, pcov, REPORT_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mITvKNH5-4fX"
      },
      "source": [
        "### Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRtUZG23mF2m"
      },
      "outputs": [],
      "source": [
        "def sample_to_md(dc):\n",
        "    s_view = dc['sample'][['OD_delta', 'plate_layout_dil', 'concentration', 'mask_reason']]\n",
        "    md = \"### **Sample: {0} '{1}' {2}**\\n\\n\".format(SAMPLE_TYPES[dc['type']], dc['type'], dc['num'])\n",
        "    md += s_view.to_markdown()\n",
        "    md += '\\n\\n'\n",
        "    md += \"CV = {:2.3} [%]  \\n\".format(100 * dc['cv'])\n",
        "    md += \"mean = {:.4} [cp/ml]  \\n\".format(dc['mean'])\n",
        "    md += \"valid = {}  \\n\".format(dc['valid'])\n",
        "    if dc['note']:\n",
        "         md += \"note: {}  \".format(dc['note'])\n",
        "\n",
        "    return md\n",
        "\n",
        "def sample_section_md(samples, img_dir):\n",
        "    md = '## Sample evaluation\\n\\n' \n",
        "    k = sample_check(samples, 'k', 1)\n",
        "    md += sample_to_md(k)\n",
        "    sfile = 'control_{0:02d}.png'.format(1)\n",
        "    img_file = path.join(img_dir, sfile)\n",
        "    sample_img(samplesk, 'k', 1, img_file, show=False)\n",
        "    md += '![\"alt text\"]({})\\n\\n'.format(sfile)\n",
        "    sample_n = samples['plate_layout_num'].astype(int).unique()\n",
        "    sample_n.sort()\n",
        "    for i in sample_n:\n",
        "        stype = 's'\n",
        "        s = sample_check(samples, stype, i)\n",
        "        md += sample_to_md(s)\n",
        "        # sample info\n",
        "        si = sample_info(samples, stype, i, verbose=False)\n",
        "        si_str = sampleinfo_to_str(si['info'])\n",
        "        if si_str:\n",
        "            md += '\\n'\n",
        "            md += 'info: ' + si_str + '  '\n",
        "        md += '\\n'\n",
        "        img_file = path.join(img_dir, 'sample_{0:02d}.png'.format(i))\n",
        "        sample_img(samplesk, stype, i, img_file=img_file, show=False, verbose=False)\n",
        "        md += '![{0}](img/{0})\\n\\n'.format(sfile)\n",
        "    return md\n",
        "\n",
        "def save_md(file_path, md_txt):\n",
        "    try:\n",
        "        with open(file_path, 'w') as fl:\n",
        "            fl.write(md_txt)\n",
        "    except Exception as e:\n",
        "        print('Error: ' + str(e))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_result = make_final()\n",
        "\n",
        "def format_results(df):\n",
        "    df.loc[:, ['Comment']] = df.apply(lambda x: final_sample_info(x['info'], x['Pre-dilution']), axis=1)\n",
        "    df.loc[:, ['CV [%]']] = df.apply(lambda x:'{:.2f}'.format(x['CV [%]']), axis=1)\n",
        "    df.loc[:, ['Result [cp/ml]']] = df.apply(lambda x: x['Comment'] if math.isnan(x['Result [cp/ml]']) else '{:.4e}'.format(x['Result [cp/ml]']), axis=1)\n",
        "\n",
        "    df.drop(['info', 'Valid', 'Reader Data [cp/ml]'], axis=1, inplace=True)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# format_results(final_result)\n",
        "# final_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comment_fn(info, multiplier):\n",
        "    return sampleinfo_to_str(info['info'], multiplier)\n",
        "\n",
        "def result_section(df):\n",
        "    # df.loc[:, ['Comment']] = df.apply(lambda x: comment_fn(x['info'], x['Pre-dilution']), axis=1)\n",
        "\n",
        "    # df.drop(['info', 'Valid'], axis=1, inplace=True)\n",
        "    md = '## Analysis Results\\n\\n'\n",
        "\n",
        "    md += format_results(df).to_markdown()\n",
        "    md += '\\n\\n'\n",
        "    \n",
        "    return md\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Header"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def header_section(date, id, msg):\n",
        "    md =  '## Header\\n\\n'\n",
        "\n",
        "    md += 'Date: {}\\n\\n'.format(date)\n",
        "    md += 'Identification: {}\\n\\n'.format(id)\n",
        "    md += 'Comment: {}\\n\\n'.format(msg)\n",
        "\n",
        "    return md;"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Report Assembly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "report = '''\n",
        "# Automatically Generated Markdown report\n",
        "\n",
        "This a PoC for automatic report generation...  \n",
        "\n",
        "'''\n",
        "\n",
        "report += header_section('05 May 2023', 'GN004240-033', 'Plate 01')\n",
        "report += result_section(final_result.drop('reference 01', axis=0))\n",
        "img_dir = path.join(REPORT_DIR, 'img')\n",
        "os.makedirs(img_dir, exist_ok=True)\n",
        "report += fit_section_md(ref, popt, pcov, img_dir) # TODO: !!! global fit_result[3]\n",
        "\n",
        "report += sample_section_md(samplesk, img_dir)\n",
        "\n",
        "save_md(REPORT_FILE_PATH, report)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aS7JK3DM_CE5"
      },
      "source": [
        "### Export to PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHf5agnXnzdp"
      },
      "outputs": [],
      "source": [
        "from md2pdf.core import md2pdf\n",
        "PDF_FILE_PATH = \"./result/my.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNCx1zronwsj"
      },
      "outputs": [],
      "source": [
        "md2pdf(PDF_FILE_PATH,\n",
        "       md_content=report,\n",
        "       md_file_path=None,\n",
        "       css_file_path=None,\n",
        "       base_url=None)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
